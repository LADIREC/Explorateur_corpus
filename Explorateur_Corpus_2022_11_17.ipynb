{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'explorateur de corpus est un script modifiable pour enrichir un corpus structuré de contenu texte. \n",
    "La forme ipynb - ou python notebook signifi que le code est organisé selon des blocs qui peuvent être roulé tous ou en partie selon les objectifs d'analyse ou l'utilisation du texte.\n",
    "Il contient des fonctions de nettoyage du texte,  de regroupement (clustering), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# environnement python 3.8.10\n",
    "\n",
    "# Le premier bloc doit etre roulé en mode administrateur\n",
    "# Installation des librairies et packages nécessaire au script\n",
    "# Cette opération ne devrait être exécutée qu'une seule fois (ou à même le terminal) \n",
    "! pip install --upgrade pip\n",
    "! pip install spacy pandas matplotlib sklearn openpyxl wget\n",
    "\n",
    "\n",
    "\n",
    "# # Téléchargement du modèle camemBERT pour le Français basé sur l'architecture roBerta\n",
    "# ! python -m wget https://dl.fbaipublicfiles.com/fairseq/models/camembert-large.tar.gz\n",
    "\n",
    "# # Décompression du fichier tar\n",
    "# ! python -m tarfile -e camembert-large.tar.gz\n",
    "\n",
    "# Intallation des packages\n",
    "! pip install flair fairseq sentencepiece transformers bitarray omegaconf\n",
    "# md pour modèle médium, sm pour modèle small et lg pour modèle large\n",
    "! python -m spacy download fr_core_news_sm  \n",
    "! python -m spacy download en_core_web_sm  \n",
    "\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ev-EHzgVNwK",
    "outputId": "c89764eb-6b45-4022-d313-0aa3b694d179"
   },
   "outputs": [],
   "source": [
    "#import des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import string as st\n",
    "import time\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des morceaux de packages \n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from spacy.lang.fr import French\n",
    "from spacy.lang.en import English\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import CamembertTokenizer\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from fairseq.models.roberta import CamembertModel\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales utilisées pour modifier les explorations des champs de texte dans les corpus\n",
    "\n",
    "\n",
    "# LISTE_COLONNE correspond à la liste des noms des colonnes sélectionnées pour les analyses, par défaut le nom des colonnes sont les mêmes que le fichier importé\n",
    "# Il est suggéré de modifié à la source s'il y a confusion. Ces nom n'ont pas à être abrévié mais ne doivent pas contenir de caractères spéciaux comme le / et les \"\n",
    "LISTE_COLONNE = ['T1','U1']\n",
    "# Dans quelle langue est le corpus ? EN pour English et FR pour Français. Si le corpus est multilingue, il doit être séparé selon la langue avant le traitement. \n",
    "# Pour ajouter de nouvelles langues, il faut un modèle de langue pour chacune (modèle, stop words, stemmer...) et ajouter aux conditions.\n",
    "LANGUE_CORPUS = 'EN'\n",
    "\n",
    "# PATH_CORPUS contient le chemin et le nom du fichier original \n",
    "PATH_CORPUS = \"corpus.xlsx\"\n",
    "\n",
    "# chargement d'un dictionnaire personnalisé d'ensembles de synonymes\n",
    "PATH_SYNSET = \"synset.xlsx\"\n",
    "\n",
    "# La liste des termes\n",
    "termes_temp = ['chef', 'chefs', 'art', 'arts', 'creativ', 'fine', 'culina', 'quality', 'spécialité', \n",
    "'gourmet', 'gourmand', 'shack', 'fried', 'fastfood', 'fast-food', 'fast food', 'tradition', 'stand', 'cantine mobile', \n",
    "'cantine', 'popote roulante', 'usine', 'travailleur', 'cuisine', 'economic', 'restaurants', 'meals', 'dishes', 'vegan', 'festival', 'foodies', 'restaurateur', 'meal']\n",
    "# La liste des thèmes \n",
    "\n",
    "aliments_temp = ['castor',  'taco', 'burger', 'hot-dog', 'poutine', 'patates', 'potato', 'fries', 'burrito', 'churros', \n",
    "'frite', 'friture', 'cheese','lentil', 'beef', 'maple', 'salad', 'pork', 'salmon', 'tartar', 'meat', 'caviar', 'fish', 'cookies', 'seafood','strawberry', 'gaufre']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chargement de la fonction de racinisation selon la langue\n",
    "print(LANGUE_CORPUS)\n",
    "if LANGUE_CORPUS == 'FR':\n",
    "    STEMMER = FrenchStemmer()\n",
    "elif LANGUE_CORPUS == 'EN':\n",
    "    STEMMER = EnglishStemmer()\n",
    "else :\n",
    "    print('erreur, il y a un problème avec la langue du corpus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALIMENTS = []\n",
    "TERMES = []\n",
    "\n",
    "for w in aliments_temp:\n",
    "    ALIMENTS.append(STEMMER.stem(w))\n",
    "print(\"Liste aliment racines:\", ALIMENTS)\n",
    "\n",
    "for w in termes_temp:\n",
    "    TERMES.append(STEMMER.stem(w))\n",
    "print(\"Liste termes racines:\", TERMES)\n",
    "\n",
    "\n",
    "display(ALIMENTS)\n",
    "display(TERMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_tree(path):\n",
    "    df1 = pd.read_excel(path, usecols=col) \n",
    "    display(df1)\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(path, col = None):\n",
    "#la fonction qui importe et créé le dataset à partir du corpus \n",
    "    df1 = pd.read_excel(path, usecols=col) \n",
    "    #par défaut, toutes le colonnes de la table sont chargé en df1\n",
    "\n",
    "    display(df1)\n",
    "   \n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_col(df, colSelect):\n",
    "# la fonction qui fusionne des colonnes selon une liste sélectionnée dans la variable globale LISTE_COLONNE.\n",
    "\n",
    "\n",
    "    df_merged = pd.DataFrame(columns=['raw', 'line'])\n",
    "    df_merged['raw'] = df[colSelect].apply(\n",
    "        lambda row: (\" \".join(row.values.astype(str))), axis=1\n",
    "    )\n",
    "    # Ajouté : df_merged[raw] qui contient la fusion des colonnes sélectionnées sans le retrait des majuscules, le df retourné contient 2 col.\n",
    "    df_merged['line'] = df[colSelect].apply(\n",
    "        lambda row: (\" \".join(row.values.astype(str)).lower()), axis=1\n",
    "    )\n",
    "\n",
    "    display(df_merged)\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "# retire la ponctuation du texte\n",
    "    translator = str.maketrans(st.punctuation, ' '*len(st.punctuation))\n",
    "    text = text.translate(translator)\n",
    "    text = text.replace(\"d'\",\" \").replace(\"l'\",\" \").replace(\"un\",\" \").replace(\"une\",\" \").replace(\"’\",\" \").replace(\"«\",\" \").replace(\"»\",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "# sépare les mots\n",
    "    text = re.split('\\s+' ,text)\n",
    "    return [x.lower() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_words(tokens):\n",
    "# selon le type de traitement et d'analyse, il peut être utile de déterminer un nombre minimum de lettre pour que le mot soit considéré.\n",
    "# pour tout garder, remplacer le nombre plus bas par un 0\n",
    "    return [x for x in tokens if len(x) > 3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "# def remove_stopwords(text):\n",
    "# la fonction va retirer du texte les termes qui sont trop courant pour avoir une signification à partir d'un dictionnaire général. \n",
    "\n",
    "    if LANGUE_CORPUS == 'FR':\n",
    "        nlp = spacy.load('fr_core_news_sm')\n",
    "    elif LANGUE_CORPUS == 'EN':\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    else :\n",
    "        print(\"il y a une problème avec la langue - stopword\")\n",
    "\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    tokens = set(tokens)\n",
    "    return list(tokens - stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_articles(text):\n",
    "# La fonction permet de restaurer les textes après le nettoyage\n",
    "    return \" \".join([word for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "# la lemmatisation ramène les mots au lemme c'est à dire l'expression générale sans accord de genre ou de nombre.\n",
    "    if LANGUE_CORPUS == 'FR':\n",
    "        nlp = spacy.load('fr_core_news_sm')\n",
    "    elif LANGUE_CORPUS == 'EN':\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    else :\n",
    "        print(\"il y a une problème avec la langue - stopword\")\n",
    "        \n",
    "    tokens = nlp(text)\n",
    "    text = \" \".join([w.lemma_ for w in tokens])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_matrix(df):\n",
    "    # les données sont mises en forme dans par sacs de mots (Bag Of Words)\n",
    "    X = 0\n",
    "    try:\n",
    "        # vectorizer = CountVectorizer(min_df = 0.15, max_df = 0.9) \n",
    "        # - Paramètres proposé originalement par Toufik = 8 dimensions, pas bon. Essayer les variations, revue littérature sur la réduction des dimensions\n",
    "        vectorizer = CountVectorizer()\n",
    "        #print(list(df['lemma_words']))\n",
    "        #print(len(list(df['lemma_words'])))\n",
    "        X = vectorizer.fit_transform(list(df['lemma_words'])).toarray()\n",
    "        #print(vectorizer.get_feature_names_out())\n",
    "        #print(len(vectorizer.get_feature_names_out()))\n",
    "        #print(len(X[0]))\n",
    "        print('\\n  >>  Transformation de données en BOW::    --    État :: '+str(u\"\\u2713\"))\n",
    "    except:\n",
    "        print('\\n  >>  Transformation de données en BOW ::    --    État :: échec')\n",
    "    return (X, vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8lO6pKPVaSY"
   },
   "outputs": [],
   "source": [
    "def get_tfidf_matrix(bow_matrix):\n",
    "  # Pondération avec une fonction de TF/IDF \n",
    "  # TF/IDF = Term frequency / Inverse document frequency afin de permettre aux mots trop ou trop peu fréquent de perdre de la valeur dans la matrice\n",
    "    try:\n",
    "        # Transformation de Sacs de mots BOW en une matrice TFIDF \n",
    "        transformer = TfidfTransformer()\n",
    "        tfidf_matrix = transformer.fit_transform(bow_matrix).toarray()\n",
    "        print(len(tfidf_matrix[0]))\n",
    "        print(len(tfidf_matrix))\n",
    "        print('\\n  >>  Transformation de BOW en une matrice TFIDF ::    --    État: '+str(u\"\\u2713\"))\n",
    "    except:\n",
    "        print('\\n  >>  Transformation de BOW en une matrice TFIDF  ::    --    État: échec')\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZPgWgTeWHpJ"
   },
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "\n",
    "    \n",
    "#    La fonction permet de trouver le nombre optimal de clusters à utiliser\n",
    "#    et dessine le graphe correspondant a chaque nombre de clusters\n",
    "\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('{} clusters'.format(k)) \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Centres de Clusters ')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE par Cluster ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dpH7rZOWYB0"
   },
   "outputs": [],
   "source": [
    "def plot_tsne_pca(data, labels):\n",
    "    \"\"\"\n",
    "    plot_tsne_pca permet la visualisation de \n",
    "    - Données haute dimensions gràce a la technique TSNE \n",
    "    - Composantes principales gràce au PCA\n",
    "    --  Les paramètres\n",
    "        @data : Les données à visualiser\n",
    "        @labels: les étiquettes de clusters \n",
    "        @return: deux graphes, un des composantes principales \n",
    "        et l'autre TNSE pour visualiser les clusters\n",
    "    \"\"\"\n",
    "    # Cette visualisation ne suffit pas pour évaluer et explorer les clusters, \n",
    "    # il faut le retour vers le corpus ajoutant une colonne au df original \n",
    "    # et on pourrait utiliser le mot le plus fréquent (les 10 mots) de ce cluster comme étiquette intelligible\n",
    "    \n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=data.shape[0])\n",
    "    pca = PCA(n_components=2).fit_transform(data[max_items,:])\n",
    "    tsne = TSNE().fit_transform(PCA(n_components=2).fit_transform(data[max_items,:]))\n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=840)\n",
    "    label_subset = labels[max_items]\n",
    "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
    "    ax[0].set_title('Clusters-PCA  Plot')\n",
    "\n",
    "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
    "    ax[1].set_title('Clusters-TNSE Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkThems(text):\n",
    "# fonction de recherche d'information par mot-clé\n",
    "    words = text.split()\n",
    "    themes_words = []\n",
    "    for w in words:\n",
    "        if STEMMER.stem(w) in ALIMENTS:\n",
    "            themes_words.append(w)\n",
    "            print(\"TESTTheme\", themes_words)\n",
    "    return themes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkChamps(text):\n",
    "    # fonction de recherche d'information par mot-clé \n",
    "    # combiner avec checkThems\n",
    "\n",
    "    words = text.split()\n",
    "    champsl = []\n",
    "    for w in words:\n",
    "        if STEMMER.stem(w) in TERMES:\n",
    "            champsl.append(w)\n",
    "            print(\"TESTChamps\", champsl)\n",
    "    return champsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchCatTree(text, df_synset):\n",
    "   # La fonction searchCatTree permet de retourner le nom d'un ensemble de synonyme ou d'une hiérarchie de catégories\"\n",
    "\n",
    "    list_words = []\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if STEMMER.stem(w) in df_synset[syn1]: ### en construction\n",
    "            list_words.append(w)\n",
    "            print(\"TESTCatTree\", w)\n",
    "    \n",
    "\n",
    "    return(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreCatTree(list_words, df):\n",
    "\n",
    " #   La fonction searchCatTree permet de retourner le la valeur d'une valeur spécifique associé à une catégorisation. \n",
    " #   Il faut faire compter les scores de chaque mots qui apparait, sommer les scores et faire la moyenne.\n",
    "\n",
    "\n",
    "    list_score = []\n",
    "    sum_score = []\n",
    "    n_score = 0\n",
    "    avr_score = 0.0\n",
    "    \n",
    "\n",
    "    return(list_score, sum_score, n_score, avr_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNer(text):\n",
    "    \"\"\"\n",
    "    La fonction getNer permet d'extraire les entités nommées \n",
    "    --  Les paramètres\n",
    "        @Text : le texte à traiter  \n",
    "        @Return: La liste des entités nommés par catégories\n",
    "\n",
    "        # Esi : il faut voir à séparer les noms et prénoms = https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md\n",
    "        # Esi : L'étiquette B-PER est associé au prénom et E-PER est associé au patronyme\n",
    "    \"\"\"\n",
    "    ner_list_per =[]\n",
    "    ner_list_loc =[]\n",
    "    ner_list_org =[]\n",
    "    ner_list_misc =[]\n",
    "    ner_list_others =[]\n",
    "    \n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        label_entity = re.sub('\\d','',str(entity.labels[0])).replace('(','').replace(')','')\n",
    "        #display(label_entity)\n",
    "        if label_entity == 'PER .':\n",
    "            ner_list_per.append(re.sub('\\d','',str(entity.tokens[0]).replace('Token: ','')).replace(' ',''))\n",
    "        elif label_entity == 'LOC .':\n",
    "            ner_list_loc.append(re.sub('\\d','',str(entity.tokens[0]).replace('Token: ','')).replace(' ',''))\n",
    "        elif label_entity == 'ORG .':\n",
    "            ner_list_org.append(re.sub('\\d','',str(entity.tokens[0]).replace('Token: ','')).replace(' ',''))\n",
    "        elif label_entity == 'MISC .':\n",
    "            ner_list_misc.append(re.sub('\\d','',str(entity.tokens[0]).replace('Token: ','')).replace(' ',''))\n",
    "        else:\n",
    "            ner_list_others.append(re.sub('\\d','',str(entity.tokens[0]).replace('Token: ','')).replace(' ',''))\n",
    "        # ner_list.append(re.sub('\\d','',str(entity.tokens[0]).replace('Token: ','')).replace(' ',''))\n",
    "        # ner_labels.append(re.sub('\\d','',str(entity.labels[0])).replace('(','').replace(')',''))\n",
    "    return {'ner_list_per': ner_list_per, 'ner_list_loc': ner_list_loc, 'ner_list_org': ner_list_org, 'ner_list_misc': ner_list_misc, 'ner_list_others': ner_list_others }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN\n",
    "Rouler ces blocs de fonctions ne devrait pas être très long. \n",
    "Ici commence l'exécution de ces fonctions sur les textes et le temps de traitement peut varier considérablement selon l'ensemble de données utilisé. Il est suggéré de faire un premier test avec un corpus réduit à 100 entrées pour estimer le temps total de traitement du corpus. Le corpus doit contenir un minimum de 10 entrées (ou le nombre de cluster min) pour ne pas avoir d'erreur.\n",
    "\n",
    "Si vous voulez sautez une fonction (par exemple le retrait des stop-words), il suffit de mettre le bloc en commentaire ou le sauter simplement à cette étape plutôt que plus haut dans le code ou les variables et fonctions sont définies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHxpnx82VhHP",
    "outputId": "6b758a37-6bd0-4f61-a8c2-5c139417d238"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH_CORPUS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\esila\\Desktop\\DEV\\Explorateur_Corpus_2022_11_17.ipynb Cellule 26\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esila/Desktop/DEV/Explorateur_Corpus_2022_11_17.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# chargement de données (corpus)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esila/Desktop/DEV/Explorateur_Corpus_2022_11_17.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# MAIN\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/esila/Desktop/DEV/Explorateur_Corpus_2022_11_17.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(PATH_CORPUS)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esila/Desktop/DEV/Explorateur_Corpus_2022_11_17.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esila/Desktop/DEV/Explorateur_Corpus_2022_11_17.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data_origin \u001b[39m=\u001b[39m get_dataset(PATH_CORPUS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PATH_CORPUS' is not defined"
     ]
    }
   ],
   "source": [
    "# chargement de données (corpus)\n",
    "# MAIN\n",
    "print(PATH_CORPUS)\n",
    "start_time = time.time()\n",
    "data_origin = get_dataset(PATH_CORPUS)\n",
    "data = merge_col(data_origin, LISTE_COLONNE)\n",
    "data[\"Doc_ID\"] = data_origin[\"Doc_ID\"]\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)\n",
    "\n",
    "\n",
    "#chargement du synset\n",
    "start_time = time.time()\n",
    "synset = get_dataset(PATH_SYNSET)\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrait de ponctuations\n",
    "start_time = time.time()\n",
    "data['removed_punc'] = data['line'].apply(lambda x: remove_punct(x))\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation\n",
    "start_time = time.time()\n",
    "data['tokens'] = data['removed_punc'].apply(lambda txt : tokenize(txt))\n",
    "# data.drop(['removed_punc'], axis=1, inplace=True)\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des tokens dont la taille < 3\n",
    "start_time = time.time()\n",
    "data['larger_tokens'] = data['tokens'].apply(lambda x : remove_small_words(x))\n",
    "# data.drop(['tokens'], axis=1, inplace=True)\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des stop words\n",
    "start_time = time.time()\n",
    "data['clean_tokens'] = data['larger_tokens'].apply(lambda x : remove_stopwords(x))\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion des tokens \n",
    "start_time = time.time()\n",
    "data['clean_text'] = data['clean_tokens'].apply(lambda x : return_articles(x))\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation \n",
    "start_time = time.time()\n",
    "data['lemma_words'] = data['clean_text'].apply(lambda x : lemmatize(x))\n",
    "# data.drop(['clean_text'], axis=1, inplace=True)\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de corpus pré-traité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le corpus nettoyé dans un fichier excel\n",
    "# à faire : réordonner les colonnes \n",
    "# + ajouter une variable globale de noms de colonnes du dataset original à ajouter aux sorties à titre de parametrage\n",
    "\n",
    "data.to_excel('corpus_nettoyé.xlsx',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour reloader le copus lémmatisé et nettoyé sans refaire le prétraitement\n",
    "#data = pd.read_excel('corpus_nettoyé.xlsx', usecols=col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation et Clustering des articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transformation de données en BOW- Chaine de traitement jusqu'à TF/IDF\n",
    "\n",
    "# bow_matrix, columns_name = get_bow_matrix(data)\n",
    "# data_bow=pd.DataFrame(bow_matrix, columns=columns_name)\n",
    "# #print(bow_matrix)\n",
    "# #print(columns_name)\n",
    "# #display(data_bow)\n",
    "# data_bow.to_excel('corpus_Bow_Matrix.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transformation de BOW en une matrice TF/IDF\n",
    "# tfidf_matrix = get_tfidf_matrix(bow_matrix)\n",
    "\n",
    "# data_tfidf=pd.DataFrame(tfidf_matrix, columns=columns_name)\n",
    "# print(tfidf_matrix)\n",
    "# display(data_tfidf)\n",
    "# data_tfidf.to_excel('corpus_TFIDF_Matrix.xlsx',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation du calcul de nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Q6atFr9MWNuu",
    "outputId": "067d3dbf-20a9-487d-c8e6-21f92963b920",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # la recherche du nombre optimal de clusters à utiliser \n",
    "# find_optimal_clusters(tfidf_matrix, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2o9UkKrWSnt"
   },
   "outputs": [],
   "source": [
    "# # Clustering\n",
    "# # Ancient paramètres données par Toufik\n",
    "# # clusters = MiniBatchKMeans(n_clusters=10, init_size=840, batch_size=200, random_state=20).fit_predict(tfidf_matrix)\n",
    "# clusters = MiniBatchKMeans(n_clusters=10, random_state=20)\n",
    "\n",
    "# clusters_tfidf = clusters.fit_predict(tfidf_matrix)\n",
    "# data['cluster_tfidt'] = clusters_tfidf\n",
    "\n",
    "# clusters_bow = clusters.fit_predict(bow_matrix)\n",
    "# data['cluster_bow'] = clusters_bow\n",
    "\n",
    "# # pour sortir le corpus avec les clusters afin de comparer les incrémentations.\n",
    "# # data.to_excel('corpus_cluster.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractions et analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "MsyfZRtfcRiP",
    "outputId": "f61389cd-2f8b-4d93-e824-4ded89dc8f66",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_tsne_pca(tfidf_matrix, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche des entités nommées\n",
    "\n",
    "C'est le loin le traitement le plus lourds, il peut s'agir de plusieurs minutes par entrée pour ce traitement selon la taille du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du Flair SequenceTagger\n",
    "if LANGUE_CORPUS == 'FR':\n",
    "    tagger = SequenceTagger.load(\"flair/ner-french\")\n",
    "elif LANGUE_CORPUS =='EN':\n",
    "    tagger = SequenceTagger.load(\"ner\")\n",
    "else:\n",
    "    print('il y a une problème avec la langue - tagger entités nommées')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction reconnaissance des entités nommées = 3min pour 12rec\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Il faut tester la qualité des résultats en fonction de la version de prétraitement : est-ce que les majuscules améliorent le nombre d'entités repérées ?\n",
    "# la colonne : data[raw] contient la fusion des colonnes sélectionnées AVEC les majuscules \n",
    "\n",
    "new_test_df = data.apply(lambda x: getNer(x.raw), axis='columns', result_type='expand')\n",
    "\n",
    "display(new_test_df)\n",
    "\n",
    "data = pd.concat([data, new_test_df], axis='columns')\n",
    "\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche d'information à partir d'une liste de thèmes\n",
    "\n",
    "start_time = time.time()\n",
    "data['Aliments'] = data['lemma_words'].apply(lambda x: checkThems(x))\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche d'information sur les termes \n",
    "\n",
    "start_time = time.time()\n",
    "# data['Termes'] = data['line'].apply(lambda x: checkChamps(x))\n",
    "data['Terme'] = data['lemma_words'].apply(lambda x: checkChamps(x))\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appeler recherche hiérarchique\n",
    "#Apply searchCatTree\n",
    "\n",
    "#Générer l'indice de centralité\n",
    "#Apply scoreCatTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT\n",
    "Ce dernier bloc est l'export du fichier de donnée enrichi. Il peut être roulé en cours de l'exploration pour s'assurer de la qualité du prétraitement et n'a pas a être roulé à la toute fin du programme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataframe sous forme MS Excel \n",
    "\n",
    "data.to_excel('corpus_enrichi.xlsx')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Acfas.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0f1dddac5913f8d8f92a00c75aa4d9652b150ed9922390442ac1744233890410"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
